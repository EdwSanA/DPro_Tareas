{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfHWjrrdOHaLQVAMVadFNF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdwSanA/DPro_Tareas/blob/main/Red_neuronal_profunda_ajustado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6E77_e1bDBs",
        "outputId": "a2465c58-e86f-4de7-d764-d8b319d93351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando MNIST...\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# Scratch Deep Neural Network - Full Implementation\n",
        "# Problemas 1 al 9\n",
        "# ===============================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 1] Capa totalmente conectada (FC)\n",
        "# ==================================================\n",
        "class FC:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        A = np.dot(X, self.W) + self.B\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        self.dW = np.dot(self.X.T, dA) / self.X.shape[0]\n",
        "        self.dB = np.mean(dA, axis=0)\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "        self = self.optimizer.update(self)\n",
        "        return dZ\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 2] Inicialización Simple\n",
        "# ==================================================\n",
        "class SimpleInitializer:\n",
        "    def __init__(self, sigma=0.01):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        return np.zeros(n_nodes2)\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 3] Optimización con SGD\n",
        "# ==================================================\n",
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return layer\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 4] Funciones de Activación\n",
        "# ==================================================\n",
        "class Tanh:\n",
        "    def forward(self, X):\n",
        "        self.Z = np.tanh(X)\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, dA):\n",
        "        return dA * (1 - self.Z**2)\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, X):\n",
        "        X = X - np.max(X, axis=1, keepdims=True)\n",
        "        exp_X = np.exp(X)\n",
        "        self.Z = exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, Z, Y):\n",
        "        batch_size = Y.shape[0]\n",
        "        return (Z - Y) / batch_size\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 5] ReLU\n",
        "# ==================================================\n",
        "class ReLU:\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dZ = dA * (self.X > 0)\n",
        "        return dZ\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 6] Xavier y He Initializers\n",
        "# ==================================================\n",
        "class XavierInitializer:\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        sigma = 1 / np.sqrt(n_nodes1)\n",
        "        return sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        return np.zeros(n_nodes2)\n",
        "\n",
        "class HeInitializer:\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        sigma = np.sqrt(2 / n_nodes1)\n",
        "        return sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        return np.zeros(n_nodes2)\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 7] AdaGrad\n",
        "# ==================================================\n",
        "class AdaGrad:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h_W, self.h_B = None, None\n",
        "\n",
        "    def update(self, layer):\n",
        "        if self.h_W is None:\n",
        "            self.h_W = np.zeros_like(layer.W)\n",
        "            self.h_B = np.zeros_like(layer.B)\n",
        "\n",
        "        self.h_W += layer.dW * layer.dW\n",
        "        self.h_B += layer.dB * layer.dB\n",
        "\n",
        "        layer.W -= self.lr * layer.dW / (np.sqrt(self.h_W) + 1e-7)\n",
        "        layer.B -= self.lr * layer.dB / (np.sqrt(self.h_B) + 1e-7)\n",
        "        return layer\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 8] Clase ScratchDeepNeuralNetrowkClassifier\n",
        "# ==================================================\n",
        "class ScratchDeepNeuralNetrowkClassifier:\n",
        "    def __init__(self, n_epochs=10, batch_size=32, verbose=True):\n",
        "        self.n_epochs = n_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.layers = []\n",
        "\n",
        "    def add_fc(self, input_dim, output_dim, activation=\"relu\", initializer=\"simple\", optimizer=\"sgd\", lr=0.01):\n",
        "        # Inicializador\n",
        "        if initializer == \"simple\":\n",
        "            init = SimpleInitializer(0.01)\n",
        "        elif initializer == \"xavier\":\n",
        "            init = XavierInitializer()\n",
        "        elif initializer == \"he\":\n",
        "            init = HeInitializer()\n",
        "        else:\n",
        "            init = SimpleInitializer(0.01)\n",
        "\n",
        "        # Optimizador\n",
        "        if optimizer == \"sgd\":\n",
        "            opt = SGD(lr)\n",
        "        elif optimizer == \"adagrad\":\n",
        "            opt = AdaGrad(lr)\n",
        "        else:\n",
        "            opt = SGD(lr)\n",
        "\n",
        "        fc = FC(input_dim, output_dim, init, opt)\n",
        "        self.layers.append(fc)\n",
        "\n",
        "        # Activación\n",
        "        if activation == \"relu\":\n",
        "            self.layers.append(ReLU())\n",
        "        elif activation == \"tanh\":\n",
        "            self.layers.append(Tanh())\n",
        "        elif activation == \"softmax\":\n",
        "            self.layers.append(Softmax())\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        for epoch in range(self.n_epochs):\n",
        "            idx = np.random.permutation(len(X))\n",
        "            X_shuffled, y_shuffled = X[idx], y[idx]\n",
        "\n",
        "            for i in range(0, len(X), self.batch_size):\n",
        "                X_batch = X_shuffled[i:i+self.batch_size]\n",
        "                y_batch = y_shuffled[i:i+self.batch_size]\n",
        "\n",
        "                # Forward\n",
        "                A = X_batch\n",
        "                for layer in self.layers:\n",
        "                    if isinstance(layer, Softmax):\n",
        "                        A = layer.forward(A)\n",
        "                    else:\n",
        "                        A = layer.forward(A)\n",
        "\n",
        "                # Backward\n",
        "                dA = self.layers[-1].backward(A, y_batch)\n",
        "                for layer in reversed(self.layers[:-1]):\n",
        "                    dA = layer.backward(dA)\n",
        "\n",
        "            if self.verbose:\n",
        "                y_pred = self.predict(X_val)\n",
        "                acc = np.mean(y_pred == np.argmax(y_val, axis=1))\n",
        "                print(f\"Epoch {epoch+1}/{self.n_epochs} - Precisión: {acc:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        A = X\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, Softmax):\n",
        "                A = layer.forward(A)\n",
        "            else:\n",
        "                A = layer.forward(A)\n",
        "        return np.argmax(A, axis=1)\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 9] Entrenamiento y Validación con MNIST\n",
        "# ==================================================\n",
        "print(\"Descargando MNIST...\")\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X = mnist.data.astype(np.float32) / 255.0\n",
        "y = mnist.target.astype(int)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X[:10000], y[:10000], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "enc = OneHotEncoder(sparse_output=False)\n",
        "Y_train_oh = enc.fit_transform(y_train.reshape(-1,1))\n",
        "Y_valid_oh = enc.transform(y_valid.reshape(-1,1))\n",
        "\n",
        "net = ScratchDeepNeuralNetrowkClassifier(n_epochs=5, batch_size=64, verbose=True)\n",
        "net.add_fc(input_dim=784, output_dim=128, activation=\"relu\", initializer=\"he\", optimizer=\"adagrad\", lr=0.01)\n",
        "net.add_fc(input_dim=128, output_dim=64, activation=\"tanh\", initializer=\"xavier\", optimizer=\"adagrad\", lr=0.01)\n",
        "#net.ad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de la red\n",
        "net = ScratchDeepNeuralNetrowkClassifier(\n",
        "    n_epochs=5, batch_size=64, verbose=True\n",
        ")\n",
        "\n",
        "net.add_fc(input_dim=784, output_dim=128, activation=\"relu\", initializer=\"he\", optimizer=\"adagrad\", lr=0.01)\n",
        "net.add_fc(input_dim=128, output_dim=64, activation=\"tanh\", initializer=\"xavier\", optimizer=\"adagrad\", lr=0.01)\n",
        "net.add_fc(input_dim=64, output_dim=10, activation=\"softmax\", initializer=\"xavier\", optimizer=\"adagrad\", lr=0.01)\n",
        "\n",
        "# Entrenamiento\n",
        "print(\"Entrenando red neuronal...\")\n",
        "net.fit(X_train, Y_train_oh, X_valid, Y_valid_oh)\n",
        "\n",
        "# Predicciones\n",
        "y_pred = net.predict(X_valid)\n",
        "\n",
        "# Precisión\n",
        "acc = np.mean(y_pred == y_valid)\n",
        "print(f\"Precisión final en validación: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQcuw-X3bv2i",
        "outputId": "5c435840-1f72-44a6-a996-b5ae6ece80d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenando red neuronal...\n",
            "Epoch 1/5 - Precisión: 0.9255\n",
            "Epoch 2/5 - Precisión: 0.9410\n",
            "Epoch 3/5 - Precisión: 0.9470\n",
            "Epoch 4/5 - Precisión: 0.9500\n",
            "Epoch 5/5 - Precisión: 0.9470\n",
            "Precisión final en validación: 0.9470\n"
          ]
        }
      ]
    }
  ]
}