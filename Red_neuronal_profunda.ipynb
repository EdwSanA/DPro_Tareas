{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+nObqPEiUSSN0u0KhVZc/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdwSanA/DPro_Tareas/blob/main/Red_neuronal_profunda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC_WyEKzgD6h",
        "outputId": "83fa78cb-139b-4cb0-ddac-8f70fa14a566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando MNIST (OpenML)...\n",
            "\n",
            "=== Entrenando configuraciones ===\n",
            "Época 01/8 | Loss tr 0.2482 | Loss va 0.2768 | Acc va 0.9137\n",
            "Época 02/8 | Loss tr 0.1168 | Loss va 0.1648 | Acc va 0.9558\n",
            "Época 03/8 | Loss tr 0.0915 | Loss va 0.1570 | Acc va 0.9575\n",
            "Época 04/8 | Loss tr 0.0623 | Loss va 0.1501 | Acc va 0.9575\n",
            "Época 05/8 | Loss tr 0.0396 | Loss va 0.1352 | Acc va 0.9604\n",
            "Época 06/8 | Loss tr 0.0265 | Loss va 0.1279 | Acc va 0.9654\n",
            "Época 07/8 | Loss tr 0.0235 | Loss va 0.1319 | Acc va 0.9604\n",
            "Época 08/8 | Loss tr 0.0136 | Loss va 0.1292 | Acc va 0.9679\n",
            "[A (ReLU/He/AdaGrad)] Accuracy validación: 0.9679\n",
            "Época 01/8 | Loss tr 2.2159 | Loss va 2.2142 | Acc va 0.2213\n",
            "Época 02/8 | Loss tr 2.1477 | Loss va 2.1440 | Acc va 0.2896\n",
            "Época 03/8 | Loss tr 2.0829 | Loss va 2.0774 | Acc va 0.3821\n",
            "Época 04/8 | Loss tr 2.0213 | Loss va 2.0140 | Acc va 0.4604\n",
            "Época 05/8 | Loss tr 1.9623 | Loss va 1.9534 | Acc va 0.5171\n",
            "Época 06/8 | Loss tr 1.9059 | Loss va 1.8953 | Acc va 0.5533\n",
            "Época 07/8 | Loss tr 1.8519 | Loss va 1.8397 | Acc va 0.5883\n",
            "Época 08/8 | Loss tr 1.8001 | Loss va 1.7865 | Acc va 0.6133\n",
            "[B (Tanh/Xavier/SGD)] Accuracy validación: 0.6133\n",
            "\n",
            "Resumen:\n",
            "A: 0.9679 | B: 0.6133\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# Scratch Deep Neural Network - Full Implementation\n",
        "# Problemas 1 al 9\n",
        "# ===============================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# OneHotEncoder compatible (scikit-learn >=1.2 y <1.2)\n",
        "try:\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    OHE_KW = {\"handle_unknown\": \"ignore\", \"sparse_output\": False}\n",
        "except TypeError:\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    OHE_KW = {\"handle_unknown\": \"ignore\", \"sparse\": False}\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 1] Capa totalmente conectada (FC)\n",
        "# ==================================================\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    Capa totalmente conectada: n_nodes1 -> n_nodes2\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "        self.optimizer = optimizer\n",
        "        # grad cache\n",
        "        self.dW = None\n",
        "        self.dB = None\n",
        "        self.X  = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        return X @ self.W + self.B\n",
        "\n",
        "    def backward(self, dA):\n",
        "        # gradientes promediados por batch\n",
        "        m = self.X.shape[0]\n",
        "        self.dW = (self.X.T @ dA) / m\n",
        "        self.dB = dA.mean(axis=0)\n",
        "        dZ = dA @ self.W.T\n",
        "        # actualizar parámetros vía optimizador\n",
        "        self = self.optimizer.update(self)\n",
        "        return dZ\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 2] Inicialización Simple\n",
        "# ==================================================\n",
        "class SimpleInitializer:\n",
        "    def __init__(self, sigma=0.01):\n",
        "        self.sigma = sigma\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "    def B(self, n_nodes2):\n",
        "        return np.zeros(n_nodes2)\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 6] Xavier y He Initializers\n",
        "# ==================================================\n",
        "class XavierInitializer:\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        sigma = 1.0 / np.sqrt(n_nodes1)\n",
        "        return sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "    def B(self, n_nodes2):\n",
        "        return np.zeros(n_nodes2)\n",
        "\n",
        "class HeInitializer:\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        sigma = np.sqrt(2.0 / n_nodes1)\n",
        "        return sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "    def B(self, n_nodes2):\n",
        "        return np.zeros(n_nodes2)\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 3] Optimizadores: SGD y [Problema 7] AdaGrad\n",
        "# ==================================================\n",
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return layer\n",
        "\n",
        "class AdaGrad:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h_W = None\n",
        "        self.h_B = None\n",
        "    def update(self, layer):\n",
        "        if self.h_W is None:\n",
        "            self.h_W = np.zeros_like(layer.W)\n",
        "            self.h_B = np.zeros_like(layer.B)\n",
        "        self.h_W += layer.dW * layer.dW\n",
        "        self.h_B += layer.dB * layer.dB\n",
        "        layer.W -= self.lr * layer.dW / (np.sqrt(self.h_W) + 1e-7)\n",
        "        layer.B -= self.lr * layer.dB / (np.sqrt(self.h_B) + 1e-7)\n",
        "        return layer\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 4] Funciones de Activación + [Problema 5] ReLU\n",
        "# ==================================================\n",
        "class Tanh:\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "    def forward(self, X):\n",
        "        self.Z = np.tanh(X)\n",
        "        return self.Z\n",
        "    def backward(self, dA):\n",
        "        return dA * (1.0 - self.Z**2)\n",
        "\n",
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "    def forward(self, X):\n",
        "        self.mask = X > 0\n",
        "        return np.maximum(0, X)\n",
        "    def backward(self, dA):\n",
        "        return dA * self.mask\n",
        "\n",
        "class Softmax:\n",
        "    \"\"\"\n",
        "    Softmax; su backward incluye Cross-Entropy:\n",
        "        dA = (softmax - y_true) / batch_size\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Z = None  # probabilidades\n",
        "    def forward(self, X):\n",
        "        Xs = X - np.max(X, axis=1, keepdims=True)\n",
        "        ex = np.exp(Xs)\n",
        "        self.Z = ex / np.sum(ex, axis=1, keepdims=True)\n",
        "        return self.Z\n",
        "    def backward(self, Z, Y_true):\n",
        "        m = Y_true.shape[0]\n",
        "        return (Z - Y_true) / m\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 8] ScratchDeepNeuralNetrowkClassifier\n",
        "# ==================================================\n",
        "class ScratchDeepNeuralNetrowkClassifier:\n",
        "    def __init__(self, n_epochs=10, batch_size=64, verbose=True):\n",
        "        self.n_epochs = n_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.layers = []\n",
        "        self.train_loss = []\n",
        "        self.val_loss = []\n",
        "\n",
        "    def add_fc(self, input_dim, output_dim, activation=\"relu\",\n",
        "               initializer=\"simple\", optimizer=\"sgd\", lr=0.01, sigma=0.01):\n",
        "        # Inicializador\n",
        "        if initializer == \"simple\":\n",
        "            init = SimpleInitializer(sigma)\n",
        "        elif initializer == \"xavier\":\n",
        "            init = XavierInitializer()\n",
        "        elif initializer == \"he\":\n",
        "            init = HeInitializer()\n",
        "        else:\n",
        "            init = SimpleInitializer(sigma)\n",
        "\n",
        "        # Optimizador\n",
        "        if optimizer == \"sgd\":\n",
        "            opt = SGD(lr)\n",
        "        elif optimizer == \"adagrad\":\n",
        "            opt = AdaGrad(lr)\n",
        "        else:\n",
        "            opt = SGD(lr)\n",
        "\n",
        "        self.layers.append(FC(input_dim, output_dim, init, opt))\n",
        "\n",
        "        # Activación posterior\n",
        "        if activation == \"relu\":\n",
        "            self.layers.append(ReLU())\n",
        "        elif activation == \"tanh\":\n",
        "            self.layers.append(Tanh())\n",
        "        elif activation == \"softmax\":\n",
        "            self.layers.append(Softmax())\n",
        "        else:\n",
        "            raise ValueError(\"Activación desconocida\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _cross_entropy(y_true, y_prob):\n",
        "        y_prob = np.clip(y_prob, 1e-7, 1 - 1e-7)\n",
        "        return -np.mean(np.sum(y_true * np.log(y_prob), axis=1))\n",
        "\n",
        "    def _forward_all(self, X):\n",
        "        A = X\n",
        "        for layer in self.layers:\n",
        "            A = layer.forward(A)\n",
        "        return A  # probabilidades si el último es Softmax\n",
        "\n",
        "    def _backward_all(self, y_true):\n",
        "        # Última capa debe ser Softmax\n",
        "        assert isinstance(self.layers[-1], Softmax)\n",
        "        dA = self.layers[-1].backward(self.layers[-1].Z, y_true)\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            dA = layer.backward(dA)\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        n = X.shape[0]\n",
        "        for epoch in range(self.n_epochs):\n",
        "            # barajar cada época\n",
        "            idx = np.random.permutation(n)\n",
        "            Xs, ys = X[idx], y[idx]\n",
        "            # mini-batches\n",
        "            for i in range(0, n, self.batch_size):\n",
        "                Xb = Xs[i:i+self.batch_size]\n",
        "                yb = ys[i:i+self.batch_size]\n",
        "                probs = self._forward_all(Xb)\n",
        "                self._backward_all(yb)\n",
        "\n",
        "            # métricas por época\n",
        "            probs_tr = self._forward_all(X)\n",
        "            loss_tr = self._cross_entropy(y, probs_tr)\n",
        "            self.train_loss.append(loss_tr)\n",
        "\n",
        "            if X_val is not None and y_val is not None:\n",
        "                probs_va = self._forward_all(X_val)\n",
        "                loss_va = self._cross_entropy(y_val, probs_va)\n",
        "                self.val_loss.append(loss_va)\n",
        "                if self.verbose:\n",
        "                    acc = (np.argmax(probs_va, axis=1) ==\n",
        "                           np.argmax(y_val, axis=1)).mean()\n",
        "                    print(f\"Época {epoch+1:02d}/{self.n_epochs} | \"\n",
        "                          f\"Loss tr {loss_tr:.4f} | Loss va {loss_va:.4f} | Acc va {acc:.4f}\")\n",
        "            elif self.verbose:\n",
        "                print(f\"Época {epoch+1:02d}/{self.n_epochs} | Loss tr {loss_tr:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self._forward_all(X)\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self._forward_all(X)\n",
        "\n",
        "# ==================================================\n",
        "# [Problema 9] Entrenamiento y Validación con MNIST\n",
        "# ==================================================\n",
        "print(\"Descargando MNIST (OpenML)...\")\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X = mnist.data.astype(np.float32) / 255.0\n",
        "y = mnist.target.astype(int)\n",
        "\n",
        "# Para que sea rápido en CPU, usamos 12k ejemplos (ajusta si quieres)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X[:12000], y[:12000], test_size=0.2, random_state=42, stratify=y[:12000]\n",
        ")\n",
        "\n",
        "enc = OneHotEncoder(**OHE_KW)\n",
        "Y_train = enc.fit_transform(y_train.reshape(-1, 1))\n",
        "Y_valid = enc.transform(y_valid.reshape(-1, 1))\n",
        "\n",
        "def run_and_report(cfg_name, layers_spec, epochs=10, batch=64, verbose=True):\n",
        "    net = ScratchDeepNeuralNetrowkClassifier(n_epochs=epochs, batch_size=batch, verbose=verbose)\n",
        "    # construir arquitectura según spec [(in,out,act,init,opt,lr)]\n",
        "    for (inp, outp, act, init, opt, lr) in layers_spec:\n",
        "        net.add_fc(inp, outp, activation=act, initializer=init, optimizer=opt, lr=lr)\n",
        "    net.fit(X_train, Y_train, X_valid, Y_valid)\n",
        "    y_pred = net.predict(X_valid)\n",
        "    acc = (y_pred == y_valid).mean()\n",
        "    print(f\"[{cfg_name}] Accuracy validación: {acc:.4f}\")\n",
        "    return acc\n",
        "\n",
        "# Configuración A: ReLU + He + AdaGrad\n",
        "cfgA = [\n",
        "    (784, 256, \"relu\",    \"he\",     \"adagrad\", 0.05),\n",
        "    (256, 128, \"relu\",    \"he\",     \"adagrad\", 0.05),\n",
        "    (128, 10,  \"softmax\", \"xavier\", \"adagrad\", 0.05),\n",
        "]\n",
        "\n",
        "# Configuración B: Tanh + Xavier + SGD\n",
        "cfgB = [\n",
        "    (784, 256, \"tanh\",    \"xavier\", \"sgd\",     0.1),\n",
        "    (256, 128, \"tanh\",    \"xavier\", \"sgd\",     0.1),\n",
        "    (128, 10,  \"softmax\", \"xavier\", \"sgd\",     0.1),\n",
        "]\n",
        "\n",
        "print(\"\\n=== Entrenando configuraciones ===\")\n",
        "accA = run_and_report(\"A (ReLU/He/AdaGrad)\", cfgA, epochs=8, batch=128, verbose=True)\n",
        "accB = run_and_report(\"B (Tanh/Xavier/SGD)\", cfgB, epochs=8, batch=128, verbose=True)\n",
        "\n",
        "print(\"\\nResumen:\")\n",
        "print(f\"A: {accA:.4f} | B: {accB:.4f}\")\n"
      ]
    }
  ]
}